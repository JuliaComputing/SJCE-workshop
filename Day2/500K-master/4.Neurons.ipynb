{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now reached one of the main subjects of the course: **neurons**.\n",
    "\n",
    "A neuron (by which we mean an \"artificial neuron\") is a caricature of a real biological neuron: it has many inputs $x_1, \\ldots, x_n$, and a single output, $y$. FIG\n",
    "\n",
    "A neuron can thus be represented as a function $f$ with inputs $x_1, \\ldots, x_n$ and single output $y$. Usually we take the function to be\n",
    "\n",
    "$$f_{w,b}(x_1, \\ldots, x_n) = \\sigma(w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b).$$\n",
    "\n",
    "Notice that this is just a more general version of the functions that we have been studying until now. The $w_1, \\ldots, w_n$ are called **weights**, and $b$ is known as the **bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a lot to write, and we don't want to write code with lots of different parameters, we collect all of the $x_1, \\ldots, x_n$ into a **vector**, and the $w$s into another vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix};\n",
    "\\qquad\n",
    "\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ \\vdots \\\\ w_n \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We thus have\n",
    "\n",
    "$$f_{\\mathbf{w}, b}(\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b),$$\n",
    "\n",
    "where the definition of the **dot product** (or scalar product, or inner product) $\\mathbf{w} \\cdot \\mathbf{x}$ is\n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x} := \\sum_i w_i x_i. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia, this becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "f(x, w, b) = σ(w ⋅ x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use a syntax in Julia that reflects very closely the mathematics.\n",
    "    (We could even use bold face if we felt like it.) Here we have implicitly assumed that the user will pass in vectors `x` and `w` to the function `f`, and that they are the same length.\n",
    "    \n",
    "[In fact, the function `f` also works with numbers, in which case it is the same as `w*x + b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999586006244"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(3, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999586006244"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ(3*4 + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `f` maps vectors of length $n$ (in $\\mathbb{R}^n$) to numbers (which, due to the definition of $\\sigma$, must lie between $0$ and $1$), only once we have chosen particular values for the weights $w_i$ and the bias $b$. How do we choose these?\n",
    "\n",
    "We will do so as before: we must define a cost function $C(\\mathbf{w}, b)$ and minimize this cost function.\n",
    "\n",
    "The cost function will be provided as follows. We wish to design our function $f$ to model some particular relationship between the input data and the output. \n",
    "As a running example, we will use our images of fruit. We could try to design a function $f$ to do the following: take in a picture of the fruit, and output a $0$ if it is an apple, and a $1$ if it is a banana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do so later. To start with, we will use summary statistics derived from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage CSV is already installed\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mMETADATA is out-of-date — you may not have the latest version of CSV\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mUse `Pkg.update()` to get the latest versions of your packages\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>height </th><th> width </th><th> red </th><th> green </th><th> blue</th></tr></thead><tbody><tr><th>1</th><td>Nullable{Int64}(98)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.583507)</td><td>Nullable{Float64}(0.500662)</td><td>Nullable{Float64}(0.207504)</td></tr><tr><th>2</th><td>Nullable{Int64}(50)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.609666)</td><td>Nullable{Float64}(0.514869)</td><td>Nullable{Float64}(0.186871)</td></tr><tr><th>3</th><td>Nullable{Int64}(52)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.610012)</td><td>Nullable{Float64}(0.515759)</td><td>Nullable{Float64}(0.187504)</td></tr><tr><th>4</th><td>Nullable{Int64}(99)</td><td>Nullable{Int64}(69)</td><td>Nullable{Float64}(0.568033)</td><td>Nullable{Float64}(0.492939)</td><td>Nullable{Float64}(0.239316)</td></tr><tr><th>5</th><td>Nullable{Int64}(51)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.609766)</td><td>Nullable{Float64}(0.51485)</td><td>Nullable{Float64}(0.183329)</td></tr><tr><th>6</th><td>Nullable{Int64}(53)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.608089)</td><td>Nullable{Float64}(0.514609)</td><td>Nullable{Float64}(0.183374)</td></tr><tr><th>7</th><td>Nullable{Int64}(53)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.608676)</td><td>Nullable{Float64}(0.515487)</td><td>Nullable{Float64}(0.186206)</td></tr><tr><th>8</th><td>Nullable{Int64}(53)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.609278)</td><td>Nullable{Float64}(0.516486)</td><td>Nullable{Float64}(0.188582)</td></tr><tr><th>9</th><td>Nullable{Int64}(99)</td><td>Nullable{Int64}(67)</td><td>Nullable{Float64}(0.569099)</td><td>Nullable{Float64}(0.494501)</td><td>Nullable{Float64}(0.241941)</td></tr><tr><th>10</th><td>Nullable{Int64}(54)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.610284)</td><td>Nullable{Float64}(0.516183)</td><td>Nullable{Float64}(0.189116)</td></tr><tr><th>11</th><td>Nullable{Int64}(54)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.607329)</td><td>Nullable{Float64}(0.515344)</td><td>Nullable{Float64}(0.188351)</td></tr><tr><th>12</th><td>Nullable{Int64}(56)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.608367)</td><td>Nullable{Float64}(0.51465)</td><td>Nullable{Float64}(0.188733)</td></tr><tr><th>13</th><td>Nullable{Int64}(56)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.606331)</td><td>Nullable{Float64}(0.513778)</td><td>Nullable{Float64}(0.18818)</td></tr><tr><th>14</th><td>Nullable{Int64}(56)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.608364)</td><td>Nullable{Float64}(0.516287)</td><td>Nullable{Float64}(0.193825)</td></tr><tr><th>15</th><td>Nullable{Int64}(57)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.605807)</td><td>Nullable{Float64}(0.513735)</td><td>Nullable{Float64}(0.191391)</td></tr><tr><th>16</th><td>Nullable{Int64}(58)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.605733)</td><td>Nullable{Float64}(0.512879)</td><td>Nullable{Float64}(0.190078)</td></tr><tr><th>17</th><td>Nullable{Int64}(59)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.604937)</td><td>Nullable{Float64}(0.513665)</td><td>Nullable{Float64}(0.192874)</td></tr><tr><th>18</th><td>Nullable{Int64}(59)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.603498)</td><td>Nullable{Float64}(0.511384)</td><td>Nullable{Float64}(0.192722)</td></tr><tr><th>19</th><td>Nullable{Int64}(59)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.600693)</td><td>Nullable{Float64}(0.510035)</td><td>Nullable{Float64}(0.189299)</td></tr><tr><th>20</th><td>Nullable{Int64}(61)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.602988)</td><td>Nullable{Float64}(0.511165)</td><td>Nullable{Float64}(0.194563)</td></tr><tr><th>21</th><td>Nullable{Int64}(61)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.600188)</td><td>Nullable{Float64}(0.509263)</td><td>Nullable{Float64}(0.193743)</td></tr><tr><th>22</th><td>Nullable{Int64}(61)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.599505)</td><td>Nullable{Float64}(0.507729)</td><td>Nullable{Float64}(0.193026)</td></tr><tr><th>23</th><td>Nullable{Int64}(63)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.597605)</td><td>Nullable{Float64}(0.507)</td><td>Nullable{Float64}(0.194595)</td></tr><tr><th>24</th><td>Nullable{Int64}(65)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.596964)</td><td>Nullable{Float64}(0.506184)</td><td>Nullable{Float64}(0.196388)</td></tr><tr><th>25</th><td>Nullable{Int64}(66)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.597058)</td><td>Nullable{Float64}(0.506745)</td><td>Nullable{Float64}(0.197678)</td></tr><tr><th>26</th><td>Nullable{Int64}(67)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.597075)</td><td>Nullable{Float64}(0.50638)</td><td>Nullable{Float64}(0.19838)</td></tr><tr><th>27</th><td>Nullable{Int64}(69)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.592933)</td><td>Nullable{Float64}(0.504191)</td><td>Nullable{Float64}(0.198419)</td></tr><tr><th>28</th><td>Nullable{Int64}(73)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.589429)</td><td>Nullable{Float64}(0.500421)</td><td>Nullable{Float64}(0.201329)</td></tr><tr><th>29</th><td>Nullable{Int64}(78)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.586179)</td><td>Nullable{Float64}(0.49841)</td><td>Nullable{Float64}(0.201781)</td></tr><tr><th>30</th><td>Nullable{Int64}(79)</td><td>Nullable{Int64}(99)</td><td>Nullable{Float64}(0.582614)</td><td>Nullable{Float64}(0.495792)</td><td>Nullable{Float64}(0.201449)</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/plain": [
       "490×5 DataFrames.DataFrame\n",
       "│ Row │ height  │  width  │  red     │  green   │  blue    │\n",
       "├─────┼─────────┼─────────┼──────────┼──────────┼──────────┤\n",
       "│ 1   │ 98      │ 99      │ 0.583507 │ 0.500662 │ 0.207504 │\n",
       "│ 2   │ 50      │ 99      │ 0.609666 │ 0.514869 │ 0.186871 │\n",
       "│ 3   │ 52      │ 99      │ 0.610012 │ 0.515759 │ 0.187504 │\n",
       "│ 4   │ 99      │ 69      │ 0.568033 │ 0.492939 │ 0.239316 │\n",
       "│ 5   │ 51      │ 99      │ 0.609766 │ 0.51485  │ 0.183329 │\n",
       "│ 6   │ 53      │ 99      │ 0.608089 │ 0.514609 │ 0.183374 │\n",
       "│ 7   │ 53      │ 99      │ 0.608676 │ 0.515487 │ 0.186206 │\n",
       "│ 8   │ 53      │ 99      │ 0.609278 │ 0.516486 │ 0.188582 │\n",
       "│ 9   │ 99      │ 67      │ 0.569099 │ 0.494501 │ 0.241941 │\n",
       "│ 10  │ 54      │ 99      │ 0.610284 │ 0.516183 │ 0.189116 │\n",
       "│ 11  │ 54      │ 99      │ 0.607329 │ 0.515344 │ 0.188351 │\n",
       "⋮\n",
       "│ 479 │ 31      │ 99      │ 0.52913  │ 0.44031  │ 0.203179 │\n",
       "│ 480 │ 37      │ 99      │ 0.528731 │ 0.456548 │ 0.230871 │\n",
       "│ 481 │ 37      │ 99      │ 0.524028 │ 0.452379 │ 0.226256 │\n",
       "│ 482 │ 37      │ 99      │ 0.523906 │ 0.452571 │ 0.230475 │\n",
       "│ 483 │ 39      │ 99      │ 0.523823 │ 0.4514   │ 0.229759 │\n",
       "│ 484 │ 39      │ 99      │ 0.522489 │ 0.449973 │ 0.233683 │\n",
       "│ 485 │ 41      │ 99      │ 0.517573 │ 0.444391 │ 0.227029 │\n",
       "│ 486 │ 41      │ 99      │ 0.515956 │ 0.441912 │ 0.230229 │\n",
       "│ 487 │ 41      │ 99      │ 0.517585 │ 0.444827 │ 0.234798 │\n",
       "│ 488 │ 41      │ 99      │ 0.510357 │ 0.436022 │ 0.228139 │\n",
       "│ 489 │ 43      │ 99      │ 0.508873 │ 0.43433  │ 0.230683 │\n",
       "│ 490 │ 31      │ 99      │ 0.528205 │ 0.440139 │ 0.199588 │"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pkg.add(\"CSV\")\n",
    "using CSV\n",
    "\n",
    "apples = CSV.read(\"apples.dat\", delim='\\t')\n",
    "bananas = CSV.read(\"bananas.dat\", delim='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These give us `DataFrames` with the data from different images.\n",
    "We will just use two data points for each image, say the columns 1 and 3, so that each data point $\\mathbf{x}^{(i)}$ is a 2-dimensional vector. We also have the label of each point as an apple or a banana, which we call $\\mathbf{y}^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neuron will take a point in the two-dimensional plane as argument and try to **classify** it as an apple ($0$) or a banana ($1$). To do so, it must \"**learn**\" the correct values of the parameters $\\mathbf{w}$ and $b$. Note that *in general we cannot expect that this is actually possible*. If it struggles, we may need a more complicated function; see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by putting all the data in a single Julia vector `x` (of which each entry is itself a vector), and the labels in a single vector `y`. [We might need to normalize column 1 to not have huge values. How is this usually dealt with?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = 3\n",
    "col2 = 4\n",
    "\n",
    "x_apples  = [ [apples[i, col1], apples[i, col2]] for i in 1:size(apples)[1] ]\n",
    "x_bananas = [ [bananas[i, col1], bananas[i, col2]] for i in 1:size(bananas)[1] ]\n",
    "\n",
    "x = vcat(x_apples, x_bananas)\n",
    "\n",
    "y = vcat( zeros(size(x_apples)[1]), ones(size(x_bananas)[1]) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the cost function accepts a single two-vector $x$ and the corresponding label $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C(params, x, y) = ( w = params[1:2]; b = params[3]; (y - f(x, w, b))^2 )\n",
    "#C(w, b) = C([w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will examine each data point in turn to try to nudge the cost function in the right direction.\n",
    "We start with *random* parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.49157 \n",
       " 0.677516\n",
       " 0.389927"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = rand(2)\n",
    "b = rand()\n",
    "\n",
    "params = [w; b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IS THIS STOCHASTIC GRADIENT DESCENT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient_descent (generic function with 2 methods)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gradient_descent(C, x, y, params, N=1000)\n",
    "\n",
    "    η = 0.01\n",
    "\n",
    "    for i in 1:N\n",
    "        \n",
    "        which = rand(1:length(x))  # choose a data point\n",
    "        \n",
    "        xx = x[which]\n",
    "        yy = y[which]\n",
    "        \n",
    "        grad = ForwardDiff.gradient(ws -> C(ws, xx, yy), params)\n",
    "        #@show grad\n",
    "        params -= η * grad\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "    \n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = [0.943811, 0.974153, 0.397488]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethodError: no method matching dot(::ForwardDiff.Dual{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3}, ::Nullable{Float64})\u001b[0m\nClosest candidates are:\n  dot(::Number, \u001b[91m::Number\u001b[39m) at linalg/generic.jl:675\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: no method matching dot(::ForwardDiff.Dual{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3}, ::Nullable{Float64})\u001b[0m\nClosest candidates are:\n  dot(::Number, \u001b[91m::Number\u001b[39m) at linalg/generic.jl:675\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mvecdot\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{ForwardDiff.Dual{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3},1}, ::Array{Nullable{Float64},1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./linalg/generic.jl:617\u001b[22m\u001b[22m",
      " [2] \u001b[1mf\u001b[22m\u001b[22m at \u001b[1m./In[1]:3\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1mC\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{ForwardDiff.Dual{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3},1}, ::Array{Nullable{Float64},1}, ::Float64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[21]:1\u001b[22m\u001b[22m",
      " [4] \u001b[1m(::##15#16{#C})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{ForwardDiff.Dual{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3},1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[24]:12\u001b[22m\u001b[22m",
      " [5] \u001b[1mvector_mode_gradient\u001b[22m\u001b[22m at \u001b[1m/Users/jane/.julia/v0.6/ForwardDiff/src/gradient.jl:94\u001b[22m\u001b[22m [inlined]",
      " [6] \u001b[1mgradient\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Array{Float64,1}, ::ForwardDiff.GradientConfig{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3,Array{ForwardDiff.Dual{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3},1}}, ::Val{true}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/jane/.julia/v0.6/ForwardDiff/src/gradient.jl:17\u001b[22m\u001b[22m",
      " [7] \u001b[1mgradient\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Array{Float64,1}, ::ForwardDiff.GradientConfig{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3,Array{ForwardDiff.Dual{ForwardDiff.Tag{##15#16{#C},Float64},Float64,3},1}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/jane/.julia/v0.6/ForwardDiff/src/gradient.jl:15\u001b[22m\u001b[22m (repeats 2 times)",
      " [8] \u001b[1mgradient_descent\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::#C, ::Array{Array{Nullable{Float64},1},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[24]:12\u001b[22m\u001b[22m",
      " [9] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "w = rand(2)\n",
    "b = rand()\n",
    "\n",
    "params = [w; b] ;\n",
    "\n",
    "@show params\n",
    "@time params = gradient_descent(C, x, y, params, 1000000)\n",
    "@show params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that we have reached a minimum of the cost function, where the gradient should be close to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: x not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: x not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1m(::##5#6)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{ForwardDiff.Dual{ForwardDiff.Tag{##5#6,Float64},Float64,3},1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[12]:1\u001b[22m\u001b[22m",
      " [2] \u001b[1mvector_mode_gradient\u001b[22m\u001b[22m at \u001b[1m/Users/jane/.julia/v0.6/ForwardDiff/src/gradient.jl:94\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1mgradient\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Array{Float64,1}, ::ForwardDiff.GradientConfig{ForwardDiff.Tag{##5#6,Float64},Float64,3,Array{ForwardDiff.Dual{ForwardDiff.Tag{##5#6,Float64},Float64,3},1}}, ::Val{true}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/jane/.julia/v0.6/ForwardDiff/src/gradient.jl:17\u001b[22m\u001b[22m",
      " [4] \u001b[1mgradient\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Array{Float64,1}, ::ForwardDiff.GradientConfig{ForwardDiff.Tag{##5#6,Float64},Float64,3,Array{ForwardDiff.Dual{ForwardDiff.Tag{##5#6,Float64},Float64,3},1}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/jane/.julia/v0.6/ForwardDiff/src/gradient.jl:15\u001b[22m\u001b[22m (repeats 2 times)",
      " [5] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    " ForwardDiff.gradient(ws -> C(ws, x[1], y[1]), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check sample data to see if the function is correctly approximated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: x not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: x not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "f(x[900], params[1:2], params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: x not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: x not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "f(x[1], params[1:2], params[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that *with sufficient training*, the single neuron is approximately able to learn the function for most of the data, but not all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: y not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: y not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "maximum(y .- f.(x, [params[1:2]], params[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.GRBackend()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots; gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: x_apples not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: x_apples not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "scatter(first.(x_apples), last.(x_apples), m=:cross)\n",
    "scatter!(first.(x_bananas), last.(x_bananas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the function that the network has learned, together with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: x_apples not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: x_apples not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "heatmap(0:0.01:1, 0:0.01:1, (x,y)->f([x,y], params[1:2], params[3]))\n",
    "\n",
    "scatter!(first.(x_apples), last.(x_apples), m=:cross)\n",
    "scatter!(first.(x_bananas), last.(x_bananas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the single neuron has *learnt* to separate the data using something that is close to a hyperplane. (Somehow we restricted the function to be a hyperplane.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
